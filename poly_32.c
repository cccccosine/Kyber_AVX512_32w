#include <stdint.h>
#include <immintrin.h>
#include <string.h>
#include "align.h"
#include "params.h"
#include "poly_32.h"
#include "ntt_32.h"
#include "consts_32.h"
#include "reduce.h"
#include "cbd.h"
#include "symmetric.h"
#include "clocks.h"

#if (KYBER_POLYCOMPRESSEDBYTES == 96)
void poly_compress(uint8_t r[96], const poly *restrict a)
{
  unsigned int i;
  __m256i f0, f1, f2, f3;
  __m128i t0, t1;
  const __m256i v = _mm256_load_si256(&qdata.vec[_16XV / 16]);
  const __m256i shift1 = _mm256_set1_epi16(1 << 8);
  const __m256i mask = _mm256_set1_epi16(7);
  const __m256i shift2 = _mm256_set1_epi16((8 << 8) + 1);
  const __m256i shift3 = _mm256_set1_epi32((64 << 16) + 1);
  const __m256i sllvdidx = _mm256_set1_epi64x(12LL << 32);
  const __m256i shufbidx = _mm256_set_epi8(8, 2, 1, 0, -1, -1, -1, -1, 14, 13, 12, 6, 5, 4, 10, 9,
                                           -1, -1, -1, -1, 14, 13, 12, 6, 5, 4, 10, 9, 8, 2, 1, 0);

  for (i = 0; i < KYBER_N / 64; i++)
  {
    f0 = _mm256_load_si256(&a->vec[4 * i + 0]);
    f1 = _mm256_load_si256(&a->vec[4 * i + 1]);
    f2 = _mm256_load_si256(&a->vec[4 * i + 2]);
    f3 = _mm256_load_si256(&a->vec[4 * i + 3]);
    f0 = _mm256_mulhi_epi16(f0, v);
    f1 = _mm256_mulhi_epi16(f1, v);
    f2 = _mm256_mulhi_epi16(f2, v);
    f3 = _mm256_mulhi_epi16(f3, v);
    f0 = _mm256_mulhrs_epi16(f0, shift1);
    f1 = _mm256_mulhrs_epi16(f1, shift1);
    f2 = _mm256_mulhrs_epi16(f2, shift1);
    f3 = _mm256_mulhrs_epi16(f3, shift1);
    f0 = _mm256_and_si256(f0, mask);
    f1 = _mm256_and_si256(f1, mask);
    f2 = _mm256_and_si256(f2, mask);
    f3 = _mm256_and_si256(f3, mask);
    f0 = _mm256_packus_epi16(f0, f1);
    f2 = _mm256_packus_epi16(f2, f3);
    f0 = _mm256_maddubs_epi16(f0, shift2); // a0 a1 a2 a3 b0 b1 b2 b3 a4 a5 a6 a7 b4 b5 b6 b7
    f2 = _mm256_maddubs_epi16(f2, shift2); // c0 c1 c2 c3 d0 d1 d2 d3 c4 c5 c6 c7 d4 d5 d6 d7
    f0 = _mm256_madd_epi16(f0, shift3);    // a0 a1 b0 b1 a2 a3 b2 b3
    f2 = _mm256_madd_epi16(f2, shift3);    // c0 c1 d0 d1 c2 c3 d2 d3
    f0 = _mm256_sllv_epi32(f0, sllvdidx);
    f2 = _mm256_sllv_epi32(f2, sllvdidx);
    f0 = _mm256_hadd_epi32(f0, f2);          // a0 c0 c0 d0 a1 b1 c1 d1
    f0 = _mm256_permute4x64_epi64(f0, 0xD8); // a0 b0 a1 b1 c0 d0 c1 d1
    f0 = _mm256_shuffle_epi8(f0, shufbidx);
    t0 = _mm256_castsi256_si128(f0);
    t1 = _mm256_extracti128_si256(f0, 1);
    t0 = _mm_blend_epi32(t0, t1, 0x08);
    _mm_storeu_si128((__m128i *)&r[24 * i + 0], t0);
    _mm_storel_epi64((__m128i *)&r[24 * i + 16], t1);
  }
}

void poly_decompress(poly *restrict r, const uint8_t a[96])
{
  unsigned int i;
  __m128i t;
  __m256i f;
  const __m256i q = _mm256_load_si256(&qdata.vec[_16XQ / 16]);
  const __m256i shufbidx = _mm256_set_epi8(5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3,
                                           2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0);
  const __m256i mask = _mm256_set_epi16(224, 28, 896, 112, 14, 448, 56, 7,
                                        224, 28, 896, 112, 14, 448, 56, 7);
  const __m256i shift = _mm256_set_epi16(128, 1024, 32, 256, 2048, 64, 512, 4096,
                                         128, 1024, 32, 256, 2048, 64, 512, 4096);

  for (i = 0; i < KYBER_N / 16; i++)
  {
    t = _mm_castps_si128(_mm_load_ss((float *)&a[6*i+0])));
    t = _mm_insert_epi16(t, *(int16_t *)&a[6 * i + 4], 2);
    f = _mm256_broadcastsi128_si256(t);
    f = _mm256_blend_epi16(f, g, 0x);
    f = _mm256_shuffle_epi8(f, shufbidx);
    f = _mm256_and_si256(f, mask);
    f = _mm256_mullo_epi16(f, shift);
    f = _mm256_mulhrs_epi16(f, q);
    _mm256_store_si256(&r->vec[i], f);
  }
}

#elif (KYBER_POLYCOMPRESSEDBYTES == 128 * 32)
void poly_compress(uint8_t r[128 * 32], const poly_32 *restrict a)
{
  unsigned int i;
  __m512i f0, f1, f2, f3;
  const __m512i v = _mm512_load_si512(&qdata_32.vec[_32XV / 32]);
  const __m512i shift1 = _mm512_set1_epi16(1 << 9);
  const __m512i mask = _mm512_set1_epi16(15);

  for (i = 0; i < KYBER_N / 4; i++)
  {
    f0 = _mm512_load_si512(&a->vec[4 * i + 0]);
    f1 = _mm512_load_si512(&a->vec[4 * i + 1]);
    f2 = _mm512_load_si512(&a->vec[4 * i + 2]);
    f3 = _mm512_load_si512(&a->vec[4 * i + 3]);
    f0 = _mm512_mulhi_epi16(f0, v);
    f1 = _mm512_mulhi_epi16(f1, v);
    f2 = _mm512_mulhi_epi16(f2, v);
    f3 = _mm512_mulhi_epi16(f3, v);
    f0 = _mm512_mulhrs_epi16(f0, shift1);
    f1 = _mm512_mulhrs_epi16(f1, shift1);
    f2 = _mm512_mulhrs_epi16(f2, shift1);
    f3 = _mm512_mulhrs_epi16(f3, shift1);
    f0 = _mm512_and_si512(f0, mask);
    f1 = _mm512_and_si512(f1, mask);
    f2 = _mm512_and_si512(f2, mask);
    f3 = _mm512_and_si512(f3, mask);

    f1 = _mm512_slli_epi16(f1, 4);
    f0 = _mm512_add_epi16(f0, f1);
    f2 = _mm512_slli_epi16(f2, 8);
    f0 = _mm512_add_epi16(f0, f2);
    f3 = _mm512_slli_epi16(f3, 12);
    f0 = _mm512_add_epi16(f0, f3);
    _mm512_storeu_si512((__m512i *)&r[64 * i], f0);
  }
}

void poly_decompress(poly_32 *restrict r, const uint8_t a[128 * 32])
{
  unsigned int i;
  __m512i f, g;
  const __m512i q = _mm512_load_si512(&qdata_32.vec[_32XQ / 32]);
  const __m512i mask = _mm512_set1_epi16(30720); //0b0111 1000 0000 0000

  for (i = 0; i < KYBER_N / 4; i++)
  {
    f = _mm512_loadu_si512((__m512i *)&a[64 * i]);
    g = _mm512_slli_epi16(f, 11);
    g = _mm512_and_si512(g, mask);
    g = _mm512_mulhrs_epi16(g, q);
    _mm512_storeu_si512((__m512i *)&r->vec[4 * i], g);
    g = _mm512_slli_epi16(f, 7);
    g = _mm512_and_si512(g, mask);
    g = _mm512_mulhrs_epi16(g, q);
    _mm512_storeu_si512((__m512i *)&r->vec[4 * i + 1], g);
    g = _mm512_slli_epi16(f, 3);
    g = _mm512_and_si512(g, mask);
    g = _mm512_mulhrs_epi16(g, q);
    _mm512_storeu_si512((__m512i *)&r->vec[4 * i + 2], g);
    g = _mm512_srli_epi16(f, 1);
    g = _mm512_and_si512(g, mask);
    g = _mm512_mulhrs_epi16(g, q);
    _mm512_storeu_si512((__m512i *)&r->vec[4 * i + 3], g);
  }
}

#elif (KYBER_POLYCOMPRESSEDBYTES == 160 * 16)
void poly_compress(uint8_t r[160 * 16], const poly_16 *restrict a)
{
  unsigned int i;
  __m256i g, f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15;
  // __m128i t0, t1;
  const __m256i v = _mm256_load_si256(&qdata_16.vec[_16XV_16 / 16]);
  const __m256i shift1 = _mm256_set1_epi16(1 << 10);
  const __m256i mask = _mm256_set1_epi16(31);
  // const __m256i shift2 = _mm256_set1_epi16((32 << 8) + 1);
  // const __m256i shift3 = _mm256_set1_epi32((1024 << 16) + 1);
  // const __m256i sllvdidx = _mm256_set1_epi64x(12);
  // const __m256i shufbidx = _mm256_set_epi8(8, -1, -1, -1, -1, -1, 4, 3, 2, 1, 0, -1, 12, 11, 10, 9,
  //                                          -1, 12, 11, 10, 9, 8, -1, -1, -1, -1, -1, 4, 3, 2, 1, 0);

  // for (i = 0; i < KYBER_N / 32; i++)
  // {
  //   f0 = _mm256_load_si256(&a->vec[2 * i + 0]);
  //   f1 = _mm256_load_si256(&a->vec[2 * i + 1]);
  //   f0 = _mm256_mulhi_epi16(f0, v);
  //   f1 = _mm256_mulhi_epi16(f1, v);
  //   f0 = _mm256_mulhrs_epi16(f0, shift1);
  //   f1 = _mm256_mulhrs_epi16(f1, shift1);
  //   f0 = _mm256_and_si256(f0, mask);
  //   f1 = _mm256_and_si256(f1, mask);
  //   f0 = _mm256_packus_epi16(f0, f1);
  //   f0 = _mm256_maddubs_epi16(f0, shift2); // a0 a1 a2 a3 b0 b1 b2 b3 a4 a5 a6 a7 b4 b5 b6 b7
  //   f0 = _mm256_madd_epi16(f0, shift3);    // a0 a1 b0 b1 a2 a3 b2 b3
  //   f0 = _mm256_sllv_epi32(f0, sllvdidx);
  //   f0 = _mm256_srlv_epi64(f0, sllvdidx);
  //   f0 = _mm256_shuffle_epi8(f0, shufbidx);
  //   t0 = _mm256_castsi256_si128(f0);
  //   t1 = _mm256_extracti128_si256(f0, 1);
  //   t0 = _mm_blendv_epi8(t0, t1, _mm256_castsi256_si128(shufbidx));
  //   _mm_storeu_si128((__m128i *)&r[20 * i + 0], t0);
  //   memcpy(&r[20 * i + 16], &t1, 4);
  // }

  for (i = 0; i < KYBER_N / 16; i++) {
    f0 = _mm256_load_si256(&a->vec[16 * i + 0]);
    f1 = _mm256_load_si256(&a->vec[16 * i + 1]);
    f2 = _mm256_load_si256(&a->vec[16 * i + 2]);
    f3 = _mm256_load_si256(&a->vec[16 * i + 3]);
    f4 = _mm256_load_si256(&a->vec[16 * i + 4]);
    f5 = _mm256_load_si256(&a->vec[16 * i + 5]);
    f6 = _mm256_load_si256(&a->vec[16 * i + 6]);
    f7 = _mm256_load_si256(&a->vec[16 * i + 7]);
    f8 = _mm256_load_si256(&a->vec[16 * i + 8]);
    f9 = _mm256_load_si256(&a->vec[16 * i + 9]);
    f10 = _mm256_load_si256(&a->vec[16 * i + 10]);
    f11 = _mm256_load_si256(&a->vec[16 * i + 11]);
    f12 = _mm256_load_si256(&a->vec[16 * i + 12]);
    f13 = _mm256_load_si256(&a->vec[16 * i + 13]);
    f14 = _mm256_load_si256(&a->vec[16 * i + 14]);
    f15 = _mm256_load_si256(&a->vec[16 * i + 15]);
    f0 = _mm256_mulhi_epi16(f0, v);
    f1 = _mm256_mulhi_epi16(f1, v);
    f2 = _mm256_mulhi_epi16(f2, v);
    f3 = _mm256_mulhi_epi16(f3, v);
    f4 = _mm256_mulhi_epi16(f4, v);
    f5 = _mm256_mulhi_epi16(f5, v);
    f6 = _mm256_mulhi_epi16(f6, v);
    f7 = _mm256_mulhi_epi16(f7, v);
    f8 = _mm256_mulhi_epi16(f8, v);
    f9 = _mm256_mulhi_epi16(f9, v);
    f10 = _mm256_mulhi_epi16(f10, v);
    f11 = _mm256_mulhi_epi16(f11, v);
    f12 = _mm256_mulhi_epi16(f12, v);
    f13 = _mm256_mulhi_epi16(f13, v);
    f14 = _mm256_mulhi_epi16(f14, v);
    f15 = _mm256_mulhi_epi16(f15, v);
    f0 = _mm256_mulhrs_epi16(f0, shift1);
    f1 = _mm256_mulhrs_epi16(f1, shift1);
    f2 = _mm256_mulhrs_epi16(f2, shift1);
    f3 = _mm256_mulhrs_epi16(f3, shift1);
    f4 = _mm256_mulhrs_epi16(f4, shift1);
    f5 = _mm256_mulhrs_epi16(f5, shift1);
    f6 = _mm256_mulhrs_epi16(f6, shift1);
    f7 = _mm256_mulhrs_epi16(f7, shift1);
    f8 = _mm256_mulhrs_epi16(f8, shift1);
    f9 = _mm256_mulhrs_epi16(f9, shift1);
    f10 = _mm256_mulhrs_epi16(f10, shift1);
    f11 = _mm256_mulhrs_epi16(f11, shift1);
    f12 = _mm256_mulhrs_epi16(f12, shift1);
    f13 = _mm256_mulhrs_epi16(f13, shift1);
    f14 = _mm256_mulhrs_epi16(f14, shift1);
    f15 = _mm256_mulhrs_epi16(f15, shift1);
    f0 = _mm256_and_si256(f0, mask);
    f1 = _mm256_and_si256(f1, mask);
    f2 = _mm256_and_si256(f2, mask);
    f3 = _mm256_and_si256(f3, mask);
    f4 = _mm256_and_si256(f4, mask);
    f5 = _mm256_and_si256(f5, mask);
    f6 = _mm256_and_si256(f6, mask);
    f7 = _mm256_and_si256(f7, mask);
    f8 = _mm256_and_si256(f8, mask);
    f9 = _mm256_and_si256(f9, mask);
    f10 = _mm256_and_si256(f10, mask);
    f11 = _mm256_and_si256(f11, mask);
    f12 = _mm256_and_si256(f12, mask);
    f13 = _mm256_and_si256(f13, mask);
    f14 = _mm256_and_si256(f14, mask);
    f15 = _mm256_and_si256(f15, mask);

    f1 = _mm256_slli_epi16(f1, 5);
    f0 = _mm256_add_epi16(f0, f1);
    f2 = _mm256_slli_epi16(f2, 10);
    f0 = _mm256_add_epi16(f0, f2);
    g = _mm256_slli_epi16(f3, 15);
    f0 = _mm256_add_epi16(f0, g);    //f0 = f3[0] | f2[4:0] | f1[4:0] | f0[4:0](以第一个16-bit为例)
    f3 = _mm256_srli_epi16(f3, 1);
    f4 = _mm256_slli_epi16(f4, 4);
    f3 = _mm256_add_epi16(f3, f4);
    f5 = _mm256_slli_epi16(f5, 9);
    f3 = _mm256_add_epi16(f3, f5);
    g = _mm256_slli_epi16(g, 14);
    f3 = _mm256_add_epi16(f3, g);    //f3 = f6[1：0] | f5[4:0] | f4[4:0] | f3[4:1]
    f6 = _mm256_srli_epi16(f6, 2);
    f7 = _mm256_slli_epi16(f7, 3);
    f6 = _mm256_add_epi16(f6, f7);
    f8 = _mm256_slli_epi16(f8, 8);
    f6 = _mm256_add_epi16(f6, f8);
    g = _mm256_slli_epi16(g, 13);
    f6 = _mm256_add_epi16(f6, g);    //f6 = f9[2：0] | f8[4:0] | f7[4:0] | f6[4:2]
    f9 = _mm256_srli_epi16(f9, 3);
    f10 = _mm256_slli_epi16(f8, 2);
    f9 = _mm256_add_epi16(f9, f10);
    f11 = _mm256_slli_epi16(f11, 7);
    f9 = _mm256_add_epi16(f9, f11);
    g = _mm256_slli_epi16(f12, 12);
    f9 = _mm256_add_epi16(f9, g);    //f9 = f12[3：0] | f11[4:0] | f10[4:0] | f9[4:3]
    f12 = _mm256_srli_epi16(f12, 4);
    f13 = _mm256_slli_epi16(f13, 1);
    f12 = _mm256_add_epi16(f12, f13);
    f14 = _mm256_slli_epi16(f14, 6);
    f12 = _mm256_add_epi16(f12, f14);
    f15 = _mm256_slli_epi16(f15, 11);
    f12 = _mm256_add_epi16(f12, f15);    //f12 = f15[4：0] | f14[4:0] | f13[4:0] | f12[4]

    _mm256_storeu_si256((__m256i *)&r[160 * i + 0], f0);
    _mm256_storeu_si256((__m256i *)&r[160 * i + 32], f0);
    _mm256_storeu_si256((__m256i *)&r[160 * i + 64], f0);
    _mm256_storeu_si256((__m256i *)&r[160 * i + 96], f0);
    _mm256_storeu_si256((__m256i *)&r[160 * i + 128], f0);

  }

}

void poly_decompress(poly_16 *restrict r, const uint8_t a[160 * 16])
{
  unsigned int i;
  // __m128i t;
  __m256i f0, f1, f2, f3, f4, g;
  // int16_t ti;
  const __m256i q = _mm256_load_si256(&qdata_16.vec[_16XQ_16 / 16]);
  const __m256i mask = _mm256_set1_epi16(31744);    //0b0111 1100 0000 0000
  // const __m256i shufbidx = _mm256_set_epi8(9, 9, 9, 8, 8, 8, 8, 7, 7, 6, 6, 6, 6, 5, 5, 5,
  //                                          4, 4, 4, 3, 3, 3, 3, 2, 2, 1, 1, 1, 1, 0, 0, 0);
  // const __m256i mask = _mm256_set_epi16(248, 1984, 62, 496, 3968, 124, 992, 31,
  //                                       248, 1984, 62, 496, 3968, 124, 992, 31);
  // const __m256i shift = _mm256_set_epi16(128, 16, 512, 64, 8, 256, 32, 1024,
  //                                        128, 16, 512, 64, 8, 256, 32, 1024);

  for (i = 0; i < KYBER_N / 16; i++)
  {
    // t = _mm_loadl_epi64((__m128i *)&a[10 * i + 0]);
    // memcpy(&ti, &a[10 * i + 8], 2);
    // t = _mm_insert_epi16(t, ti, 4);
    // f = _mm256_broadcastsi128_si256(t);
    // f = _mm256_shuffle_epi8(f, shufbidx);
    // f = _mm256_and_si256(f, mask);
    // f = _mm256_mullo_epi16(f, shift);
    // f = _mm256_mulhrs_epi16(f, q);
    // _mm256_store_si256(&r->vec[i], f);

    f0 = _mm256_loadu_si256((__m256i *)&a[160 * i + 0]);
    f1 = _mm256_loadu_si256((__m256i *)&a[160 * i + 32]);
    f2 = _mm256_loadu_si256((__m256i *)&a[160 * i + 64]);
    f3 = _mm256_loadu_si256((__m256i *)&a[160 * i + 96]);
    f4 = _mm256_loadu_si256((__m256i *)&a[160 * i + 128]);

    g = _mm256_slli_epi16(f0, 10);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 0], g);
    g = _mm256_slli_epi16(f0, 5);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 1], g);
    g = _mm256_and_si256(f0, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 2], g);
    f0 = _mm256_srli_epi16(f0, 5);
    g = _mm256_slli_epi16(f1, 11);
    f0 = _mm256_add_epi16(f0, g);
    g = _mm256_and_si256(f0, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 3], g);
    g = _mm256_slli_epi16(f1, 6);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 4], g);
    g = _mm256_slli_epi16(f1, 1);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 5], g);
    f1 = _mm256_srli_epi16(f1, 4);
    g = _mm256_slli_epi16(f2, 12);
    f1 = _mm256_add_epi16(f1, g);
    g = _mm256_and_si256(f1, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 6], g);
    g = _mm256_slli_epi16(f2, 7);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 7], g);
    g = _mm256_slli_epi16(f2, 2);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 8], g);
    f2 = _mm256_srli_epi16(f2, 3);
    g = _mm256_slli_epi16(f3, 13);
    f2 = _mm256_add_epi16(f2, g);
    g = _mm256_and_si256(f2, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 9], g);
    g = _mm256_slli_epi16(f3, 8);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 10], g);
    g = _mm256_slli_epi16(f3, 3);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 11], g);
    f3 = _mm256_srli_epi16(f3, 2);
    g = _mm256_slli_epi16(f4, 14);
    f3 = _mm256_add_epi16(f3, g);
    g = _mm256_and_si256(f3, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 12], g);
    g = _mm256_slli_epi16(f4, 9);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 13], g);
    g = _mm256_slli_epi16(f4, 4);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 14], g);
    g = _mm256_srli_epi16(f4, 1);
    g = _mm256_and_si256(g, mask);
    g = _mm256_mulhrs_epi16(g, q);
    _mm256_storeu_si256((__m256i *)&r->vec[16 * i + 15], g);
    
  }
}

#endif

void poly_tobytes(uint8_t r[KYBER_POLYBYTES * 32], const poly_32 *a)
{
  ntttobytes_avx512_32(r, a->vec, qdata_32.vec);
}

void poly_frombytes(poly_32 *r, const uint8_t a[KYBER_POLYBYTES * 32])
{
  nttfrombytes_avx512_32(r->vec, a, qdata_32.vec);
}

void poly_frommsg_32(poly_32 *restrict r, const uint8_t msg[KYBER_INDCPA_MSGBYTES * 32])
{
#if (KYBER_INDCPA_MSGBYTES != 32)
#error "KYBER_INDCPA_MSGBYTES must be equal to 32!"
#endif
  __m512i f, g0, g1, g2, g3;
  const __m512i idx1 = _mm512_set_epi8(15, 15, 14, 14, 13, 13, 12, 12, 11, 11, 10, 10, 9, 9, 8, 8, 7, 7, 6, 6, 5, 5, 4, 4, 3, 3, 2, 2, 1, 1, 0, 0, 
  15, 15, 14, 14, 13, 13, 12, 12, 11, 11, 10, 10, 9, 9, 8, 8, 7, 7, 6, 6, 5, 5, 4, 4, 3, 3, 2, 2, 1, 1, 0, 0);

  const __m512i hqs = _mm512_set1_epi16((KYBER_Q + 1) / 2);

#define FROMMSG64(i)                       \
  g0 = _mm512_shuffle_i64x2(f, f, 0x50);   \
  g1 = _mm512_shuffle_i64x2(f, f, 0xFA);   \
  g2 = _mm512_shuffle_epi8(g0, idx1);      \
  g3 = _mm512_shuffle_epi8(g1, idx1);      \
  g0 = _mm512_slli_epi16(g2, 15);          \
  g1 = _mm512_slli_epi16(g3, 15);          \
  g0 = _mm512_srai_epi16(g0, 15);          \
  g1 = _mm512_srai_epi16(g1, 15);          \
  g0 = _mm512_and_si512(g0, hqs);          \
  g1 = _mm512_and_si512(g1, hqs);          \
  _mm512_store_si512(&r->vec[i], g0);      \
  _mm512_store_si512(&r->vec[i + 8], g1);  \
  g0 = _mm512_slli_epi16(g2, 14);          \
  g1 = _mm512_slli_epi16(g3, 14);          \
  g0 = _mm512_srai_epi16(g0, 15);          \
  g1 = _mm512_srai_epi16(g1, 15);          \
  g0 = _mm512_and_si512(g0, hqs);          \
  g1 = _mm512_and_si512(g1, hqs);          \
  _mm512_store_si512(&r->vec[i + 1], g0);  \
  _mm512_store_si512(&r->vec[i + 9], g1);  \
  g0 = _mm512_slli_epi16(g2, 13);          \
  g1 = _mm512_slli_epi16(g3, 13);          \
  g0 = _mm512_srai_epi16(g0, 15);          \
  g1 = _mm512_srai_epi16(g1, 15);          \
  g0 = _mm512_and_si512(g0, hqs);          \
  g1 = _mm512_and_si512(g1, hqs);          \
  _mm512_store_si512(&r->vec[i + 2], g0);  \
  _mm512_store_si512(&r->vec[i + 10], g1); \
  g0 = _mm512_slli_epi16(g2, 12);          \
  g1 = _mm512_slli_epi16(g3, 12);          \
  g0 = _mm512_srai_epi16(g0, 15);          \
  g1 = _mm512_srai_epi16(g1, 15);          \
  g0 = _mm512_and_si512(g0, hqs);          \
  g1 = _mm512_and_si512(g1, hqs);          \
  _mm512_store_si512(&r->vec[i + 3], g0);  \
  _mm512_store_si512(&r->vec[i + 11], g1); \
  g0 = _mm512_slli_epi16(g2, 11);          \
  g1 = _mm512_slli_epi16(g3, 11);          \
  g0 = _mm512_srai_epi16(g0, 15);          \
  g1 = _mm512_srai_epi16(g1, 15);          \
  g0 = _mm512_and_si512(g0, hqs);          \
  g1 = _mm512_and_si512(g1, hqs);          \
  _mm512_store_si512(&r->vec[i + 4], g0);  \
  _mm512_store_si512(&r->vec[i + 12], g1); \
  g0 = _mm512_slli_epi16(g2, 10);          \
  g1 = _mm512_slli_epi16(g3, 10);          \
  g0 = _mm512_srai_epi16(g0, 15);          \
  g1 = _mm512_srai_epi16(g1, 15);          \
  g0 = _mm512_and_si512(g0, hqs);          \
  g1 = _mm512_and_si512(g1, hqs);          \
  _mm512_store_si512(&r->vec[i + 5], g0);  \
  _mm512_store_si512(&r->vec[i + 13], g1); \
  g0 = _mm512_slli_epi16(g2, 9);           \
  g1 = _mm512_slli_epi16(g3, 9);           \
  g0 = _mm512_srai_epi16(g0, 15);          \
  g1 = _mm512_srai_epi16(g1, 15);          \
  g0 = _mm512_and_si512(g0, hqs);          \
  g1 = _mm512_and_si512(g1, hqs);          \
  _mm512_store_si512(&r->vec[i + 6], g0);  \
  _mm512_store_si512(&r->vec[i + 14], g1); \
  g0 = _mm512_slli_epi16(g2, 8);           \
  g1 = _mm512_slli_epi16(g3, 8);           \
  g0 = _mm512_srai_epi16(g0, 15);          \
  g1 = _mm512_srai_epi16(g1, 15);          \
  g0 = _mm512_and_si512(g0, hqs);          \
  g1 = _mm512_and_si512(g1, hqs);          \
  _mm512_store_si512(&r->vec[i + 7], g0);  \
  _mm512_store_si512(&r->vec[i + 15], g1);

  f = _mm512_loadu_si512((__m512i *)msg); // the structure of msg is a(0)0-a(31)0, a(0)1-a(31)1,...., a(0)31-a(31)31, in which a(0)0 is 8-bit
  FROMMSG64(0);
  f = _mm512_loadu_si512((__m512i *)(msg + 64));
  FROMMSG64(16);
  f = _mm512_loadu_si512((__m512i *)(msg + 128));
  FROMMSG64(32);
  f = _mm512_loadu_si512((__m512i *)(msg + 192));
  FROMMSG64(48);
  f = _mm512_loadu_si512((__m512i *)(msg + 256));
  FROMMSG64(64);
  f = _mm512_loadu_si512((__m512i *)(msg + 320));
  FROMMSG64(80);
  f = _mm512_loadu_si512((__m512i *)(msg + 384));
  FROMMSG64(96);
  f = _mm512_loadu_si512((__m512i *)(msg + 448));
  FROMMSG64(112);
  f = _mm512_loadu_si512((__m512i *)(msg + 512));
  FROMMSG64(128);
  f = _mm512_loadu_si512((__m512i *)(msg + 576));
  FROMMSG64(144);
  f = _mm512_loadu_si512((__m512i *)(msg + 640));
  FROMMSG64(160);
  f = _mm512_loadu_si512((__m512i *)(msg + 704));
  FROMMSG64(176);
  f = _mm512_loadu_si512((__m512i *)(msg + 768));
  FROMMSG64(192);
  f = _mm512_loadu_si512((__m512i *)(msg + 832));
  FROMMSG64(208);
  f = _mm512_loadu_si512((__m512i *)(msg + 896));
  FROMMSG64(224);
  f = _mm512_loadu_si512((__m512i *)(msg + 960));
  FROMMSG64(240);
}

void poly_tomsg_32(uint8_t msg[KYBER_INDCPA_MSGBYTES * 32], const poly_32 *restrict a)
{
  unsigned int i;
  __m512i f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15,
      g0, g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11, g12, g13, g14, g15;
  const __m512i hq = _mm512_set1_epi16((KYBER_Q - 1) / 2);
  const __m512i hhq = _mm512_set1_epi16((KYBER_Q - 1) / 4);
  const __m512i mask = _mm512_set1_epi16(0b1000000010000000); // 0b1000 0000 1000 0000
  const __m512i idx = _mm512_set_epi64(7, 5, 3, 1, 6, 4, 2, 0);

  for (i = 0; i < KYBER_N/16; i++)
  {
    f0 = _mm512_load_si512(&a->vec[16 * i + 0]);
    f1 = _mm512_load_si512(&a->vec[16 * i + 1]);
    f2 = _mm512_load_si512(&a->vec[16 * i + 2]);
    f3 = _mm512_load_si512(&a->vec[16 * i + 3]);
    f4 = _mm512_load_si512(&a->vec[16 * i + 4]);
    f5 = _mm512_load_si512(&a->vec[16 * i + 5]);
    f6 = _mm512_load_si512(&a->vec[16 * i + 6]);
    f7 = _mm512_load_si512(&a->vec[16 * i + 7]);
    f8 = _mm512_load_si512(&a->vec[16 * i + 8]);
    f9 = _mm512_load_si512(&a->vec[16 * i + 9]);
    f10 = _mm512_load_si512(&a->vec[16 * i + 10]);
    f11 = _mm512_load_si512(&a->vec[16 * i + 11]);
    f12 = _mm512_load_si512(&a->vec[16 * i + 12]);
    f13 = _mm512_load_si512(&a->vec[16 * i + 13]);
    f14 = _mm512_load_si512(&a->vec[16 * i + 14]);
    f15 = _mm512_load_si512(&a->vec[16 * i + 15]);

    f0 = _mm512_sub_epi16(hq, f0);
    f1 = _mm512_sub_epi16(hq, f1);
    f2 = _mm512_sub_epi16(hq, f2);
    f3 = _mm512_sub_epi16(hq, f3);
    f4 = _mm512_sub_epi16(hq, f4);
    f5 = _mm512_sub_epi16(hq, f5);
    f6 = _mm512_sub_epi16(hq, f6);
    f7 = _mm512_sub_epi16(hq, f7);
    f8 = _mm512_sub_epi16(hq, f8);
    f9 = _mm512_sub_epi16(hq, f9);
    f10 = _mm512_sub_epi16(hq, f10);
    f11 = _mm512_sub_epi16(hq, f11);
    f12 = _mm512_sub_epi16(hq, f12);
    f13 = _mm512_sub_epi16(hq, f13);
    f14 = _mm512_sub_epi16(hq, f14);
    f15 = _mm512_sub_epi16(hq, f15);

    g0 = _mm512_srai_epi16(f0, 15);
    g1 = _mm512_srai_epi16(f1, 15);
    g2 = _mm512_srai_epi16(f2, 15);
    g3 = _mm512_srai_epi16(f3, 15);
    g4 = _mm512_srai_epi16(f4, 15);
    g5 = _mm512_srai_epi16(f5, 15);
    g6 = _mm512_srai_epi16(f6, 15);
    g7 = _mm512_srai_epi16(f7, 15);
    g8 = _mm512_srai_epi16(f8, 15);
    g9 = _mm512_srai_epi16(f9, 15);
    g10 = _mm512_srai_epi16(f10, 15);
    g11 = _mm512_srai_epi16(f11, 15);
    g12 = _mm512_srai_epi16(f12, 15);
    g13 = _mm512_srai_epi16(f13, 15);
    g14 = _mm512_srai_epi16(f14, 15);
    g15 = _mm512_srai_epi16(f15, 15);

    f0 = _mm512_xor_si512(f0, g0);
    f1 = _mm512_xor_si512(f1, g1);
    f2 = _mm512_xor_si512(f2, g2);
    f3 = _mm512_xor_si512(f3, g3);
    f4 = _mm512_xor_si512(f4, g4);
    f5 = _mm512_xor_si512(f5, g5);
    f6 = _mm512_xor_si512(f6, g6);
    f7 = _mm512_xor_si512(f7, g7);
    f8 = _mm512_xor_si512(f8, g8);
    f9 = _mm512_xor_si512(f9, g9);
    f10 = _mm512_xor_si512(f10, g10);
    f11 = _mm512_xor_si512(f11, g11);
    f12 = _mm512_xor_si512(f12, g12);
    f13 = _mm512_xor_si512(f13, g13);
    f14 = _mm512_xor_si512(f14, g14);
    f15 = _mm512_xor_si512(f15, g15);

    f0 = _mm512_sub_epi16(f0, hhq);
    f1 = _mm512_sub_epi16(f1, hhq);
    f2 = _mm512_sub_epi16(f2, hhq);
    f3 = _mm512_sub_epi16(f3, hhq);
    f4 = _mm512_sub_epi16(f4, hhq);
    f5 = _mm512_sub_epi16(f5, hhq);
    f6 = _mm512_sub_epi16(f6, hhq);
    f7 = _mm512_sub_epi16(f7, hhq);
    f8 = _mm512_sub_epi16(f8, hhq);
    f9 = _mm512_sub_epi16(f9, hhq);
    f10 = _mm512_sub_epi16(f10, hhq);
    f11 = _mm512_sub_epi16(f11, hhq);
    f12 = _mm512_sub_epi16(f12, hhq);
    f13 = _mm512_sub_epi16(f13, hhq);
    f14 = _mm512_sub_epi16(f14, hhq);
    f15 = _mm512_sub_epi16(f15, hhq);

    g0 = _mm512_packs_epi16(f0, f8);
    g1 = _mm512_packs_epi16(f1, f9);
    g2 = _mm512_packs_epi16(f2, f10);
    g3 = _mm512_packs_epi16(f3, f11);
    g4 = _mm512_packs_epi16(f4, f12);
    g5 = _mm512_packs_epi16(f5, f13);
    g6 = _mm512_packs_epi16(f6, f14);
    g7 = _mm512_packs_epi16(f7, f15);

    g0 = _mm512_permutexvar_epi64(idx, g0);
    g1 = _mm512_permutexvar_epi64(idx, g1);
    g2 = _mm512_permutexvar_epi64(idx, g2);
    g3 = _mm512_permutexvar_epi64(idx, g3);
    g4 = _mm512_permutexvar_epi64(idx, g4);
    g5 = _mm512_permutexvar_epi64(idx, g5);
    g6 = _mm512_permutexvar_epi64(idx, g6);
    g7 = _mm512_permutexvar_epi64(idx, g7);

    g0 = _mm512_and_si512(g0, mask);
    g1 = _mm512_and_si512(g1, mask);
    g2 = _mm512_and_si512(g2, mask);
    g3 = _mm512_and_si512(g3, mask);
    g4 = _mm512_and_si512(g4, mask);
    g5 = _mm512_and_si512(g5, mask);
    g6 = _mm512_and_si512(g6, mask);
    g7 = _mm512_and_si512(g7, mask);

    g0 = _mm512_srli_epi16(g0, 7);
    g1 = _mm512_srli_epi16(g1, 6);
    g2 = _mm512_srli_epi16(g2, 5);
    g3 = _mm512_srli_epi16(g3, 4);
    g4 = _mm512_srli_epi16(g4, 3);
    g5 = _mm512_srli_epi16(g5, 2);
    g6 = _mm512_srli_epi16(g6, 1);

    g0 = _mm512_add_epi16(g0, g1);
    g0 = _mm512_add_epi16(g0, g2);
    g0 = _mm512_add_epi16(g0, g3);
    g0 = _mm512_add_epi16(g0, g4);
    g0 = _mm512_add_epi16(g0, g5);
    g0 = _mm512_add_epi16(g0, g6);
    g0 = _mm512_add_epi16(g0, g7);

    memcpy(&msg[64 * i], &g0, 64);
  }
}

/*  目前用不上
void poly_getnoise_eta1(poly *r, const uint8_t seed[KYBER_SYMBYTES], uint8_t nonce)
{
  ALIGNED_UINT8(KYBER_ETA1 * KYBER_N / 4 + 32)
  buf; // +32 bytes as required by poly_cbd_eta1
  prf(buf.coeffs, KYBER_ETA1 * KYBER_N / 4, seed, nonce);
  poly_cbd_eta1(r, buf.vec);
}
*/

void poly_getnoise_eta2(poly *r, const uint8_t seed[KYBER_SYMBYTES], uint8_t nonce)
{
  ALIGNED_UINT8(KYBER_ETA2 * KYBER_N / 4)
  buf;
  prf(buf.coeffs, KYBER_ETA2 * KYBER_N / 4, seed, nonce);
  poly_cbd_eta2(r, buf.vec);
}

#ifndef KYBER_90S
#define NOISE_NBLOCKS ((KYBER_ETA1 * KYBER_N / 4 + SHAKE256_RATE - 1) / SHAKE256_RATE)
void poly_getnoise_eta1_8x(poly_32 *r0,
                           poly_32 *r1,
                           poly_32 *r2,
                           poly_32 *r3,
                           poly_32 *r4,
                           poly_32 *r5,
                           poly_32 *r6,
                           poly_32 *r7,
                           uint8_t seed[32*(32*2-1)],
                           uint8_t nonce0,
                           uint8_t nonce1,
                           uint8_t nonce2,
                           uint8_t nonce3,
                           uint8_t nonce4,
                           uint8_t nonce5,
                           uint8_t nonce6,
                           uint8_t nonce7)
{
  ALIGNED_UINT8(NOISE_NBLOCKS * SHAKE256_RATE) buf[8]; // NOISE_NBLOCKS = 1
  __m256i f;
  keccakx8_state state;

  for(int i = 0; i < 32; i++) {
    f = _mm256_loadu_si256((__m256i *)(seed+32*i*2));
    _mm256_store_si256(buf[0].vec, f);
    _mm256_store_si256(buf[1].vec, f);
    _mm256_store_si256(buf[2].vec, f);
    _mm256_store_si256(buf[3].vec, f);
    _mm256_store_si256(buf[4].vec, f);
    _mm256_store_si256(buf[5].vec, f);
    _mm256_store_si256(buf[6].vec, f);
    _mm256_store_si256(buf[7].vec, f);

    buf[0].coeffs[32] = nonce0;
    buf[1].coeffs[32] = nonce1;
    buf[2].coeffs[32] = nonce2;
    buf[3].coeffs[32] = nonce3;
    buf[4].coeffs[32] = nonce4;
    buf[5].coeffs[32] = nonce5;
    buf[6].coeffs[32] = nonce6;
    buf[7].coeffs[32] = nonce7;

    shake256x8_absorb_once(&state, buf[0].coeffs, buf[1].coeffs, buf[2].coeffs, buf[3].coeffs, buf[4].coeffs, buf[5].coeffs, buf[6].coeffs, buf[7].coeffs, 33);
    shake256x8_squeezeblocks(buf[0].coeffs, buf[1].coeffs, buf[2].coeffs, buf[3].coeffs, buf[4].coeffs, buf[5].coeffs, buf[6].coeffs, buf[7].coeffs, NOISE_NBLOCKS, &state);

    poly_cbd_eta1(r0->coeffs+i*KYBER_N, buf[0].vec);
    poly_cbd_eta1(r1->coeffs+i*KYBER_N, buf[1].vec);
    poly_cbd_eta1(r2->coeffs+i*KYBER_N, buf[2].vec);
    poly_cbd_eta1(r3->coeffs+i*KYBER_N, buf[3].vec);
    poly_cbd_eta1(r4->coeffs+i*KYBER_N, buf[4].vec);
    poly_cbd_eta1(r5->coeffs+i*KYBER_N, buf[5].vec);
    poly_cbd_eta1(r6->coeffs+i*KYBER_N, buf[6].vec);
    poly_cbd_eta1(r7->coeffs+i*KYBER_N, buf[7].vec);
  }
  
}

#define NOISE_NBLOCKS2 ((KYBER_ETA2 * KYBER_N / 4 + SHAKE256_RATE - 1) / SHAKE256_RATE)
void poly_getnoise_eta2_4x(poly_32 *r, const uint8_t seed[KYBER_SYMBYTES*(16*2-1)], uint8_t nonce)
{
  // ALIGNED_UINT8(KYBER_ETA2 * KYBER_N / 4)
  // buf;
  // prf(buf.coeffs, KYBER_ETA2 * KYBER_N / 4, seed, nonce);
  // poly_cbd_eta2(r, buf.vec);

  ALIGNED_UINT8(NOISE_NBLOCKS2 * SHAKE256_RATE) buf[4]; // NOISE_NBLOCKS = 1
  __m256i f;
  keccakx8_state state;

  for(int i = 0; i < 4; i++) {
    f = _mm256_loadu_si256((__m256i *)(seed+32*(i*4)*2));
    _mm256_store_si256(buf[0].vec, f);
    f = _mm256_loadu_si256((__m256i *)(seed+32*(i*4+1)*2));
    _mm256_store_si256(buf[1].vec, f);
    f = _mm256_loadu_si256((__m256i *)(seed+32*(i*4+2)*2));
    _mm256_store_si256(buf[2].vec, f);
    f = _mm256_loadu_si256((__m256i *)(seed+32*(i*4+3)*2));
    _mm256_store_si256(buf[3].vec, f);

    buf[0].coeffs[32] = nonce;
    buf[1].coeffs[32] = nonce;
    buf[2].coeffs[32] = nonce;
    buf[3].coeffs[32] = nonce;

    //这里暂时未改，先注释使得可以make通过
    // shake256x4_absorb_once(&state, buf[0].coeffs, buf[1].coeffs, buf[2].coeffs, buf[3].coeffs, 33);
    // shake256x4_squeezeblocks(buf[0].coeffs, buf[1].coeffs, buf[2].coeffs, buf[3].coeffs, NOISE_NBLOCKS, &state);

    poly_cbd_eta1(r->coeffs+(i*4)*KYBER_N, buf[0].vec);
    poly_cbd_eta1(r->coeffs+(i*4+1)*KYBER_N, buf[1].vec);
    poly_cbd_eta1(r->coeffs+(i*4+2)*KYBER_N, buf[2].vec);
    poly_cbd_eta1(r->coeffs+(i*4+3)*KYBER_N, buf[3].vec);
  }

}

#if KYBER_K == 2
void poly_getnoise_eta1122_4x(poly_16 *r0,
                              poly_16 *r1,
                              poly_16 *r2,
                              poly_16 *r3,
                              const uint8_t seed[32*(16*2-1)],
                              uint8_t nonce0,
                              uint8_t nonce1,
                              uint8_t nonce2,
                              uint8_t nonce3)
{
  ALIGNED_UINT8(NOISE_NBLOCKS * SHAKE256_RATE)
  buf[4];
  __m256i f;
  keccakx4_state state;

  for(int i = 0; i < 16; i++) {
    f = _mm256_loadu_si256((__m256i *)(seed+32*i*2));
    _mm256_store_si256(buf[0].vec, f);
    _mm256_store_si256(buf[1].vec, f);
    _mm256_store_si256(buf[2].vec, f);
    _mm256_store_si256(buf[3].vec, f);

    buf[0].coeffs[32] = nonce0;
    buf[1].coeffs[32] = nonce1;
    buf[2].coeffs[32] = nonce2;
    buf[3].coeffs[32] = nonce3;

    shake256x4_absorb_once(&state, buf[0].coeffs, buf[1].coeffs, buf[2].coeffs, buf[3].coeffs, 33);
    shake256x4_squeezeblocks(buf[0].coeffs, buf[1].coeffs, buf[2].coeffs, buf[3].coeffs, NOISE_NBLOCKS, &state);

    poly_cbd_eta1(r0->coeffs+i*KYBER_N, buf[0].vec);
    poly_cbd_eta1(r1->coeffs+i*KYBER_N, buf[1].vec);
    poly_cbd_eta2(r2->coeffs+i*KYBER_N, buf[2].vec);
    poly_cbd_eta2(r3->coeffs+i*KYBER_N, buf[3].vec);
  }
  
}



#endif
#endif

void poly_ntt(poly_32 *r)
{
  ntt_avx_32(r->vec, qdata_32.vec);
}

void poly_invntt_tomont(poly_32 *r)
{
  invntt_avx_32(r->vec, qdata_32.vec);
}

void poly_basemul_montgomery(poly_32 *r, const poly_32 *a, const poly_32 *b)
{
  basemul_avx_32(r->vec, a->vec, b->vec, qdata_32.vec);
}

void poly_tomont(poly_32 *r)
{
  tomont_avx_32(r->vec, qdata_32.vec);
}

void poly_reduce(poly_32 *r)
{
  reduce_avx_32(r->vec, qdata_32.vec);
}

void poly_add(poly_32 *r, const poly_32 *a, const poly_32 *b)
{
  unsigned int i = 0;
  __m512i f0, f1;

  // uint64_t start, end, cost[KYBER_N];
  // _mm_prefetch(&a->vec[i + 2], _MM_HINT_T0);
  // _mm_prefetch(&b->vec[i + 2], _MM_HINT_T0);
  // f0 = _mm256_load_si256(&a->vec[i]);
  // f1 = _mm256_load_si256(&b->vec[i]);
  // f0 = _mm256_add_epi16(f0, f1);
  // _mm256_store_si256(&r->vec[i], f0);
  // i++;
  // f0 = _mm256_load_si256(&a->vec[i]);
  // f1 = _mm256_load_si256(&b->vec[i]);
  // f0 = _mm256_add_epi16(f0, f1);
  // _mm256_store_si256(&r->vec[i], f0);
  // i++;
  // printf("a: %p, b: %p, a_addr: %p, b_addr: %p\n", &a->vec[i], &b->vec[i], a_addr & mask, b_addr & mask);
  // _mm_prefetch(a_addr & mask, _MM_HINT_T0);
  // _mm_prefetch(b_addr & mask, _MM_HINT_T0);
  // _mm_prefetch((a_addr & mask) + 4096, _MM_HINT_T0);
  // _mm_prefetch((b_addr & mask) + 4096, _MM_HINT_T0);
  // _mm_prefetch((a_addr & mask) + 4096 * 2, _MM_HINT_T0);
  // _mm_prefetch((b_addr & mask) + 4096 * 2, _MM_HINT_T0);
  // _mm_prefetch((a_addr & mask) + 4096 * 3, _MM_HINT_T0);
  // _mm_prefetch((b_addr & mask) + 4096 * 3, _MM_HINT_T0);
  // uint64_t a_addr = &a->vec[i];
  // uint64_t b_addr = &b->vec[i];
  // uint64_t mask = 0xfffffffffffff000;

  // f0 = _mm256_load_si256((a_addr & mask));
  // f1 = _mm256_load_si256((b_addr & mask));
  // f0 = _mm256_add_epi16(f0, f1);
  // _mm256_store_si256((a_addr & mask), f0);

  // f0 = _mm256_load_si256((a_addr & mask) + 4096);
  // f1 = _mm256_load_si256((b_addr & mask) + 4096);
  // f0 = _mm256_add_epi16(f0, f1);
  // _mm256_store_si256((a_addr & mask) + 4096, f0);

  // f0 = _mm256_load_si256((a_addr & mask) + 4096 * 2);
  // f1 = _mm256_load_si256((b_addr & mask) + 4096 * 2);
  // f0 = _mm256_add_epi16(f0, f1);
  // _mm256_store_si256((a_addr & mask) + 4096 * 2, f0);

  // f0 = _mm256_load_si256((a_addr & mask) + 4096 * 3);
  // f1 = _mm256_load_si256((b_addr & mask) + 4096 * 3);
  // f0 = _mm256_add_epi16(f0, f1);
  // _mm256_store_si256((a_addr & mask) + 4096 * 3, f0);

  for (i = 0; i < KYBER_N; i++)
  {
    // _mm_prefetch(&a->vec[i + 2], _MM_HINT_T0);
    // _mm_prefetch(&b->vec[i + 2], _MM_HINT_T0);
    // start = cycles_now();
    f0 = _mm512_load_si512(&a->vec[i]);
    f1 = _mm512_load_si512(&b->vec[i]);
    f0 = _mm512_add_epi16(f0, f1);
    _mm512_store_si512(&r->vec[i], f0);
    // end = cycles_now();
    // cost[i] = end - start;
  }

  // for (i = 0; i < KYBER_N; i++)
  // {
  //   printf("%d: %d\n", i, cost[i]);
  // }

}

void poly_sub(poly_32 *r, const poly_32 *a, const poly_32 *b)
{
  unsigned int i;
  __m512i f0, f1;

  for (i = 0; i < KYBER_N; i++)
  {
    f0 = _mm512_load_si512(&a->vec[i]);
    f1 = _mm512_load_si512(&b->vec[i]);
    f0 = _mm512_sub_epi16(f0, f1);
    _mm512_store_si512(&r->vec[i], f0);
  }
}
